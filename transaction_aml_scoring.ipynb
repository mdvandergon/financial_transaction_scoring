{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transaction_aml_scoring.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNNKUymDQnPGWXTpVZa0t9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdvandergon/financial_transaction_scoring/blob/main/transaction_aml_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2xjwxaa9IOx"
      },
      "source": [
        "# Transaction AML Scoring\n",
        "Creating an AML fraud classifier using BoostedTrees and a transaction graph embedding.\n",
        "\n",
        "This project uses sample data from IBM's AMLSim project. You can find their repo here: https://github.com/IBM/AMLSim/\n",
        "\n",
        "They also have a wiki page about the data.\n",
        "https://github.com/IBM/AMLSim/wiki/Data-Schema-for-Input-Parameters-and-Generated-Data-Set#transactions-transactionscsv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2MPKRcbsOnE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDV-Ecw_si8e"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR5DTpAj9vLX"
      },
      "source": [
        "### Step 0 - get the environment set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9mZuqPz6qT8"
      },
      "source": [
        "! pip install --user --upgrade numpy\"<1.19.0,>=1.16.0\" powerlaw python-dateutil plotly plotly_express==0.4.1 pandas tensorflow==2.3.1 umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzWnmhA48-5h"
      },
      "source": [
        "## Step 1 - Get the IBM AML Transaction Data\n",
        "\n",
        "There is a way to generate it, but they also have some sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSPGuF8BHpVb"
      },
      "source": [
        "# example data is available on Dropbox :)\n",
        "! wget https://www.dropbox.com/sh/l3grpumqfgbxqak/AAC8YT4fdn0AYKhyZ5b3Ax16a?dl=1 -O aml.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOanN_Yf_2s"
      },
      "source": [
        "! unzip aml.zip -d data/\n",
        "! echo \"DONE!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DQS3AohK6lD"
      },
      "source": [
        "# 7 zip (apt install p7zip)\n",
        "! p7zip -d data/100vertices-10Kedges.7z\n",
        "! p7zip -d data/1Mvertices-100Medges.7z\n",
        "# if you don't have the space, you can use this medium dataset\n",
        "# ! p7zip -d data/10Kvertices-1Medges.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF50dNcEWO0_"
      },
      "source": [
        "# The work begins..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob5a0uBd9tE6"
      },
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import json\n",
        "import umap\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn.neighbors\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBawwg6HzeeR"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMHqRq_nSEJz"
      },
      "source": [
        "# create a co-occurance matrix where sender, receiver pairs are tallied up\n",
        "def create_cooccurance_matrix(df: pd.DataFrame, col_0: str, col_1: str, normed=False):\n",
        "  n = max(df[col_0].max(), df[col_1].max())\n",
        "  mtx = np.zeros((n + 1, n + 1))\n",
        "  for i, row in df.iterrows():\n",
        "    s = row[col_0]\n",
        "    d = row[col_1]\n",
        "    mtx[s,d] += 1\n",
        "  if normed:\n",
        "    mtx = mtx / np.linalg.norm(mtx)\n",
        "  return mtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ItxqDDeqAh1"
      },
      "source": [
        "MODEL = umap.UMAP()\n",
        "def embed(x: int, model: umap.UMAP = MODEL):\n",
        "  return model.embedding_[x]\n",
        "\n",
        "def get_embedding_features(df: pd.DataFrame, col_0: str, col_1: str, model=MODEL,\n",
        "                           embed_suffix ='_EMBED', dst_col='EMBED_DISTANCE'):\n",
        "  \n",
        "  for col in [col_0, col_1]:\n",
        "    df.loc[: , col + embed_suffix] = df[col].apply(lambda x: embed(x, model=model))\n",
        "    # split out components as features\n",
        "    for c in range(n_components):\n",
        "      df.loc[:, col + f'{embed_suffix}_{c}'] = df[col + embed_suffix].apply(lambda x: x[c])\n",
        "\n",
        "  # compute the cosine distance: float\n",
        "  us = df[col_0 + embed_suffix].values\n",
        "  vs = df[col_1 + embed_suffix].values\n",
        "  cos = np.array([distance.cosine(u, v) for u,v in zip(us, vs)])\n",
        "  df.loc[:, dst_col] = cos\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sll_CZVAWWP"
      },
      "source": [
        "# based on the data investigation, these are our VIP columns\n",
        "src_col = 'SENDER_ACCOUNT_ID'\n",
        "dst_col = 'RECEIVER_ACCOUNT_ID'\n",
        "TARGET_COL = 'IS_FRAUD'\n",
        "LABEL_COL = 'ALERT_ID'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPiZvZxAbhR"
      },
      "source": [
        "## Step 2 - Transaction EDA and Graph Embedding with UMAP\n",
        "\n",
        "Refer to this link for data dictionary: https://github.com/IBM/AMLSim/wiki/Data-Schema-for-Input-Parameters-and-Generated-Data-Set#transactions-transactionscsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6IoxUbAWMl"
      },
      "source": [
        "transactions_path = '100vertices-10Kedges/transactions.csv'\n",
        "sample_df = pd.read_csv(transactions_path)\n",
        "print(sample_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEj-b3JYAWPQ"
      },
      "source": [
        "sample_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fPH0Bty0PWw"
      },
      "source": [
        "# transaction types\n",
        "sample_df[TARGET_COL].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i33bb61rQ8h4"
      },
      "source": [
        "co_mtx = create_cooccurance_matrix(sample_df, src_col, dst_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8G1UkOyHHHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3dFHzMtkE7R"
      },
      "source": [
        "# create a label for each account_id was in a IS_FRAUD observation\n",
        "n = max(sample_df[src_col].max(), sample_df[dst_col].max())\n",
        "fraudulent = sample_df[(sample_df[TARGET_COL] == True)]\n",
        "fraud_parties = pd.concat([fraudulent[src_col], fraudulent[dst_col]])\n",
        "fraud_parties = set(fraud_parties.values.ravel())  # unsorted list of parties in a fraudulent transaction\n",
        "fraud_label = np.array([1 if i in fraud_parties else 0 for i in range(n+1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm4aWWWvk5Ki"
      },
      "source": [
        "### UMAP embedding to approximate local area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-StGyxTJNm"
      },
      "source": [
        "n_components = 3\n",
        "metric = 'correlation' # hellinger, correlation\n",
        "model = umap.UMAP(n_components=n_components,\n",
        "                  metric=metric)\n",
        "embedding = model.fit(co_mtx)\n",
        "# umap.plot.points(mapper, values=np.arange(100000), theme='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVDLIQGui3iY"
      },
      "source": [
        "# Optional: you can use this to do KNN clustering (-1 is an outlier)\n",
        "# outlier_scores = sklearn.neighbors.LocalOutlierFactor(contamination=0.001428).fit_predict(embedding.embedding_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9er5yku1lkTD"
      },
      "source": [
        "px.scatter_3d(x=embedding.embedding_[:,0], y=embedding.embedding_[:,1], z=embedding.embedding_[:,2], color=fraud_label,\n",
        "              title=f\"Tx Graph Embedding on small AMLSim dataset ({metric})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiOLrcqAps7-"
      },
      "source": [
        "There is a time-varying UMAP available that would be worth investigating:\n",
        "https://umap-learn.readthedocs.io/en/latest/aligned_umap_politics_demo.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZOmBvUWD9wk"
      },
      "source": [
        "# other ideas:\n",
        "\n",
        "# 1- Aligned UMAP\n",
        "# create a relation map, which is just a map from ACCOUNT_ID to an id\n",
        "# ids = set(np.concatenate([sample_df[src_col].values, sample_df[dst_col].values])) \n",
        "# relation_dict = {x:i for i, x in enumerate(ids)}\n",
        "\n",
        "# 2 - there might be an accelerated way to do the co-occurance matrix, but I am too tired\n",
        "# encoded_src = tf.keras.utils.to_categorical(df[src_col])\n",
        "# encoded_dst = tf.keras.utils.to_categorical(df[dst])\n",
        "# create an co-occurance matrix\n",
        "# df_asint = df.astype(int)\n",
        "# coocc = df_asint.T.dot(df_asint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu708eIUtmrQ"
      },
      "source": [
        "### Turn our embedding into a feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEB_5XRqq8yU"
      },
      "source": [
        "get_embedding_features(sample_df.head(), src_col, dst_col, model=embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItBADk2mAkH-"
      },
      "source": [
        "## Step 3 - Tensorflow model training and benchmark against linear\n",
        "\n",
        "**Targets: Y**\n",
        "\n",
        "1) IS_FRAUD: binary (derived from ALERT_ID, categorical)\n",
        "\n",
        "**Features: X**\n",
        "\n",
        "1) TX_AMOUNT: float \n",
        "\n",
        "2) TX_TYPE: categorical\n",
        "\n",
        "3) SENDER_ACCOUNT_ID_EMBED (several columns)\n",
        "\n",
        "4) RECEIVER_ACCOUNT_ID_EMBED (several columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCt-U-XAAddB"
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrodxSnd3x4F"
      },
      "source": [
        "# switch to the big-ole dataset\n",
        "transactions_path = '10Kvertices-1Medges/transactions.csv'\n",
        "df = pd.read_csv(transactions_path)\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htg99oB0ZXpZ"
      },
      "source": [
        "# train/eval/test split\n",
        "# FYI: If we want to predict the alert code, we can't have negative labels, so a null label will be \"0\" instead of -1\n",
        "# df.loc[:, LABEL_COL] = df.loc[:, LABEL_COL] + 1\n",
        "# simulate a cut point for a test set\n",
        "test_idx = int(df.shape[0] * 0.1)\n",
        "# (train/eval)/test split:\n",
        "df_test = df.iloc[-test_idx:, :]\n",
        "X_test = df_test.drop(TARGET_COL, axis=1)\n",
        "y_test = df_test[TARGET_COL].astype(int)\n",
        "\n",
        "# train/eval\n",
        "df_ = df.iloc[:test_idx, :]\n",
        "X = df_.drop(TARGET_COL, axis=1)\n",
        "y = df_[TARGET_COL].astype(int)\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4eomyS31Bh1"
      },
      "source": [
        "# # upsample fraudlent tx\n",
        "# recombine\n",
        "d = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# separate data using target column\n",
        "not_fraud = d[d[TARGET_COL] == 0]\n",
        "fraud = d[d[TARGET_COL] == 1]\n",
        "\n",
        "# upsample IS_FRAUD\n",
        "n_samples =  d.shape[0] // 10 # roughly approx prec-recall tradeoff for a business: look at 10% of transactions\n",
        "fraud_upsampled = resample(fraud,\n",
        "                          replace=True,\n",
        "                          n_samples=n_samples,\n",
        "                          random_state=42)\n",
        "\n",
        "# recreate training set\n",
        "d_train = pd.concat([not_fraud, fraud_upsampled])\n",
        "X_train = d_train.drop(TARGET_COL, axis=1)\n",
        "y_train = d_train[TARGET_COL]\n",
        "\n",
        "# check new class counts\n",
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnhf0k7z5_M4"
      },
      "source": [
        "# obtain our embeddings without leaking information from our eval or test set\n",
        "n_components = 3\n",
        "metric = 'euclidean' # hellinger, euclidean, correlation\n",
        "print(\"building matrix...\")\n",
        "co_mtx = create_cooccurance_matrix(X_train, src_col, dst_col)\n",
        "print(\"training embedding...\")\n",
        "train_embedding = umap.UMAP(n_components=n_components,\n",
        "                            metric=metric).fit(co_mtx)\n",
        "print(\"done. applying embeddings to train\")\n",
        "X_train = get_embedding_features(X_train, src_col, dst_col, model=train_embedding)\n",
        "\n",
        "# experimental - create a feature based on local outliers (KNN)\n",
        "# print(\"predicting outliers...\")\n",
        "# src_outlier_feature_col = 'SRC_OUTLIER_SCORE'\n",
        "# dst_outlier_feature_col = 'DST_OUTLIER_SCORE'\n",
        "# clf = sklearn.neighbors.LocalOutlierFactor(contamination=0.001428)\n",
        "# _scores = clf.fit(embedding.embedding_)  # fit_predict returns a -1, 1\n",
        "# outlier_scores_src = [clf.negative_outlier_factor_[i] for i in X_train[src_col].values]  # get a score for each sender_id\n",
        "# outlier_scores_dst = [clf.negative_outlier_factor_[i] for i in X_train[dst_col].values]\n",
        "# X_train.loc[:, src_outlier_feature_col] = outlier_scores_src\n",
        "# X_train.loc[:, dst_outlier_feature_col] = outlier_scores_dst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz-2Uo-eb5bO"
      },
      "source": [
        "src_embed_cols = [f'SENDER_ACCOUNT_ID_EMBED_{c}' for c in range(n_components)]\n",
        "dst_embed_cols = [f'RECEIVER_ACCOUNT_ID_EMBED_{c}' for c in range(n_components)]\n",
        "color_scale = color_continuous_scale=px.colors.cmocean.matter\n",
        "px.scatter_3d(x=X_train[src_embed_cols[0]],\n",
        "              y=X_train[src_embed_cols[1]],\n",
        "              z=X_train[src_embed_cols[2]],\n",
        "              opacity=0.5,\n",
        "              color_continuous_scale=color_scale,\n",
        "              color=y_train.astype(float),\n",
        "              title=f\"Embedding Space ({metric})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlOdVOooql3p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iknle0xnu1ON"
      },
      "source": [
        "# apply the embedding model to the eval set\n",
        "print(\"getting embed on eval set...\")\n",
        "X_eval = get_embedding_features(X_eval, src_col, dst_col, model=train_embedding)\n",
        "\n",
        "# Test -- could run the embedding on both train/eval, but this is quicker\n",
        "print(\"getting embed on test set...\")\n",
        "X_test = get_embedding_features(X_test, src_col, dst_col, model=train_embedding)\n",
        "# X_test = X_test[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2BVDLXQBfKF"
      },
      "source": [
        "# filter by cols we want to model\n",
        "# to learn more about feature columns: https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
        "\n",
        "src_embed_cols = [f'SENDER_ACCOUNT_ID_EMBED_{c}' for c in range(n_components)]\n",
        "dst_embed_cols = [f'RECEIVER_ACCOUNT_ID_EMBED_{c}' for c in range(n_components)]\n",
        "CATEGORICAL_COLUMNS = [src_col]  # could also have TX_TYPE, but it only has one value\n",
        "NUMERIC_COLUMNS = ['TX_AMOUNT', 'EMBED_DISTANCE'] + src_embed_cols + dst_embed_cols\n",
        "\n",
        "def one_hot_cat_column(feature_name, vocab):\n",
        "  return tf.feature_column.indicator_column(\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\n",
        "                                                 vocab))\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  # Need to one-hot encode categorical features.\n",
        "  vocabulary = X_train[feature_name].unique()\n",
        "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,\n",
        "                                           dtype=tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKkYNlbskmI-"
      },
      "source": [
        "# drop cols\n",
        "X_train = X_train[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS]\n",
        "X_eval = X_eval[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS]\n",
        "X_test= X_test[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EPuWSMk9sWu"
      },
      "source": [
        "X_train.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4fC31P9A1Q"
      },
      "source": [
        "# training description\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(y_train.name, \"\\n--- on: ---\\n\", \"\\n\".join(list(X_train.columns)))\n",
        "print(y_train.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vesDF8B_BwwR"
      },
      "source": [
        "# 5 batches\n",
        "NUM_EXAMPLES = len(y_train)\n",
        "EXP_PER_BATCH = 5\n",
        "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
        "  def input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n",
        "    if shuffle:\n",
        "      dataset = dataset.shuffle(NUM_EXAMPLES)\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).\n",
        "    dataset = dataset.repeat(n_epochs)\n",
        "    # In memory training doesn't use batching (ie batch === num_examples)\n",
        "    dataset = dataset.batch(NUM_EXAMPLES // EXP_PER_BATCH)\n",
        "    return dataset\n",
        "  return input_fn\n",
        "\n",
        "# Training and evaluation input functions.\n",
        "train_input_fn = make_input_fn(X_train, y_train)\n",
        "eval_input_fn = make_input_fn(X_eval, y_eval, shuffle=False, n_epochs=1)\n",
        "test_input_fn = make_input_fn(X_test, y_test, shuffle=False, n_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG4_2mMzIinO"
      },
      "source": [
        "### train a baseline linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBqheECHB0uc"
      },
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns)\n",
        "\n",
        "# Train model.\n",
        "linear_est.train(train_input_fn, max_steps=100)\n",
        "\n",
        "# Evaluation.\n",
        "result = linear_est.evaluate(eval_input_fn)\n",
        "print(pd.Series(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKifu_XDB9OK"
      },
      "source": [
        "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
        "\n",
        "probs.plot(kind='hist', bins=50, title='predicted probabilities')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioDBeKf5B_al"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_eval, probs)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.xlim(0,)\n",
        "plt.ylim(0,)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGooiK9iEIXY"
      },
      "source": [
        "Tensorflow Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ezIbRHnD7o1"
      },
      "source": [
        "! rm -rf bt_cls/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATNbq3ovB3J1"
      },
      "source": [
        "# Above one batch is defined as the entire dataset.\n",
        "model_dir = 'bt_cls'\n",
        "n_batches = 5 # the whole dataset\n",
        "max_depth = 4\n",
        "l2_reg = 1e-8\n",
        "max_steps = 100\n",
        "# prune_mode = 'post'\n",
        "# tree_complexity = 1e-4\n",
        "est = tf.estimator.BoostedTreesClassifier(feature_columns, \n",
        "                                          max_depth=max_depth,\n",
        "                                          l2_regularization=l2_reg,\n",
        "                                          n_batches_per_layer=n_batches,\n",
        "                                          model_dir=model_dir)\n",
        "\n",
        "# The model will stop training once the specified number of trees is built, not\n",
        "# based on the number of steps.\n",
        "est.train(train_input_fn, max_steps=max_steps)\n",
        "\n",
        "# Eval.\n",
        "result = est.evaluate(eval_input_fn)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0vV9oDQbnkb"
      },
      "source": [
        "### BoostedTree evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGurb6rTCBcJ"
      },
      "source": [
        "pred_dicts = list(est.predict(eval_input_fn))\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
        "\n",
        "probs.plot(kind='hist', bins=50, title='predicted probabilities')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UShfA_UzF2aK"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_eval, probs)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate')\n",
        "plt.xlim(0,)\n",
        "plt.ylim(0,)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqCiDIJK0_hj"
      },
      "source": [
        "importances = est.experimental_feature_importances(normalize=True)\n",
        "df_imp = pd.Series(importances)\n",
        "\n",
        "# Visualize importances\n",
        "N = X_train.shape[1]\n",
        "ax = (df_imp.iloc[0:N][::-1]\n",
        "    .plot(kind='barh',\n",
        "          color='blue',\n",
        "          title='Gain feature importances',\n",
        "          figsize=(10, 6)))\n",
        "ax.grid(False, axis='y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j11Q8EFn2o6m"
      },
      "source": [
        "color_scale = color_continuous_scale=px.colors.cmocean.matter\n",
        "px.scatter_3d(x=X_eval[src_embed_cols[0]],\n",
        "              y=X_eval[src_embed_cols[1]],\n",
        "              z=X_eval[src_embed_cols[2]],\n",
        "              opacity=0.5,\n",
        "              color_continuous_scale=color_scale,\n",
        "              color=probs.values, text=[\"fraud\" if x else \"\" for x in y_eval.values],\n",
        "              title=\"Needles in a Haystack: Tx Graph Embedding on AMLSim dataset (red = potential AML)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LWG6oVRSmE0"
      },
      "source": [
        "px.scatter(x=X_eval['EMBED_DISTANCE'],\n",
        "              y=probs,\n",
        "              opacity=0.8,\n",
        "              color_continuous_scale=color_scale,\n",
        "              color=y_eval.astype(int),\n",
        "              title=\"Needles in a Haystack: Graph distance for targets is close\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT23dCDv-FJE"
      },
      "source": [
        "px.scatter(x=X_eval['TX_AMOUNT'],\n",
        "              y=probs,\n",
        "              opacity=0.8,\n",
        "              color_continuous_scale=color_scale,\n",
        "              color=y_eval.astype(int),\n",
        "              title=\"(Small) Needles in a Haystack: fradulent transactions are small\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDKmuOcfjkcf"
      },
      "source": [
        "# VaR = probs * X_eval['TX_AMOUNT'].values  # Expected Value at Risk\n",
        "# px.histogram(x=VaR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctc8188IeGMd"
      },
      "source": [
        "## Test set results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCKq_TIH-SjD"
      },
      "source": [
        "result = linear_est.evaluate(test_input_fn)\n",
        "print(\"Linear Cls Test Set results\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu15lUqtEjjg"
      },
      "source": [
        "result = est.evaluate(test_input_fn)\n",
        "print(\"Boosted Tress Cls Test Set results\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Zj3d08OFpd"
      },
      "source": [
        "# Could train the Co-Occurance Matrix on train and eval. \n",
        "# Because this was all generated by the same process, I don't think it is necessary\n",
        "# co_mtx = create_cooccurance_matrix(pd.concat([X_train, X_eval]), src_col, dst_col)\n",
        "# emb = umap.UMAP(n_components=n_components).fit(co_mtx)\n",
        "# df_test = get_embeddings(df_test, src_col, dst_col, model=emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9imakNNOr7Ns"
      },
      "source": [
        "### Export to Saved Model\n",
        "\n",
        "for use later via: https://www.tensorflow.org/api_docs/python/tf/saved_model/load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbDQcOQfr6g0"
      },
      "source": [
        "# This is mysterious, but this example made it easy: https://www.tensorflow.org/lattice/tutorials/canned_estimators#creating_input_fn\n",
        "srv_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        feature_spec=tf.feature_column.make_parse_example_spec(feature_columns))\n",
        "est.export_saved_model('saved_model', srv_fn, as_text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEfmplFc87Gx"
      },
      "source": [
        "save..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSe093XK8c6e"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unr3CXWS89yD"
      },
      "source": [
        "export_num = 1606669696  # check the output above to see this number\n",
        "path = f'saved_model/{export_num}'\n",
        "export_name = 'transaction_scorer.zip'\n",
        "\n",
        "with zipfile.ZipFile(export_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "  for root, dirs, filepaths in os.walk(path):\n",
        "      for f in filepaths:\n",
        "          zipf.write(os.path.join(root, f))\n",
        "\n",
        "files.download(export_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPkhdvwjAMTI"
      },
      "source": [
        "# download the graph embedding\n",
        "embed_name = 'transaction_graph.npy'\n",
        "np.save(embed_name, train_embedding.embedding_)\n",
        "files.download(embed_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOk_EIy589ny"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ8qfpqFGRo1"
      },
      "source": [
        "### FYI: Porting to AWS Sagemaker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2FbRcjT4FqG"
      },
      "source": [
        "# run on AWS\n",
        "# https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#prepare-a-training-script\n",
        "# aml_estimator = TensorFlow(entry_point='aml_cls.py',\n",
        "#                              role=role,\n",
        "#                              train_instance_count=2,\n",
        "#                              train_instance_type='ml.p3.2xlarge',\n",
        "#                              framework_version='2.3.0',\n",
        "#                              py_version='py3',\n",
        "#                              distributions={'parameter_server': {'enabled': True}})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}